{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bf0bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[7]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>14.66</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>[A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[10]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.47</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>[B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Johny Johny Yes Papa\"[18]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>6.92</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bath Song\"[19]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.75</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"See You Again\"[20]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>6.30</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>[C]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Rank  \\\n",
       "0       \"Baby Shark Dance\"[7]   \n",
       "1             \"Despacito\"[10]   \n",
       "2  \"Johny Johny Yes Papa\"[18]   \n",
       "3             \"Bath Song\"[19]   \n",
       "4         \"See You Again\"[20]   \n",
       "\n",
       "                                                Name Artist       Upload Date  \\\n",
       "0        Pinkfong Baby Shark - Kids' Songs & Stories  14.66     June 17, 2016   \n",
       "1                                         Luis Fonsi   8.47  January 12, 2017   \n",
       "2  LooLoo Kids - Nursery Rhymes and Children's Songs   6.92   October 8, 2016   \n",
       "3                         Cocomelon - Nursery Rhymes   6.75       May 2, 2018   \n",
       "4                                        Wiz Khalifa   6.30     April 6, 2015   \n",
       "\n",
       "  Views  \n",
       "0   [A]  \n",
       "1   [B]  \n",
       "2        \n",
       "3        \n",
       "4   [C]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Question 1- most viewed videos on YouTube\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table in the page\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize lists to store data\n",
    "ranks = []\n",
    "names = []\n",
    "artists = []\n",
    "upload_dates = []\n",
    "views = []\n",
    "\n",
    "# Extract data from the table\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    ranks.append(cells[0].text.strip() if len(cells) > 0 else '-')\n",
    "    names.append(cells[1].text.strip() if len(cells) > 1 else '-')\n",
    "    artists.append(cells[2].text.strip() if len(cells) > 2 else '-')\n",
    "    upload_dates.append(cells[3].text.strip() if len(cells) > 3 else '-')\n",
    "    views.append(cells[4].text.strip() if len(cells) > 4 else '-')\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Rank': ranks,\n",
    "    'Name': names,\n",
    "    'Artist': artists,\n",
    "    'Upload Date': upload_dates,\n",
    "    'Views': views\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3f0ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Series, Place, Date, Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 2 - URL of the BCCI website\n",
    "base_url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Request the home page\n",
    "response = requests.get(base_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the home page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_page_link = None\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if 'international' in a['href']:\n",
    "        fixtures_page_link = a['href']\n",
    "        break\n",
    "\n",
    "# If the fixtures page link is found, request that page\n",
    "if fixtures_page_link:\n",
    "    fixtures_url = base_url + fixtures_page_link\n",
    "    fixtures_response = requests.get(fixtures_url)\n",
    "    fixtures_response.raise_for_status()\n",
    "    \n",
    "    # Parse the fixtures page content\n",
    "    fixtures_soup = BeautifulSoup(fixtures_response.content, 'html.parser')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    series = []\n",
    "    places = []\n",
    "    dates = []\n",
    "    times = []\n",
    "    \n",
    "    # Find and extract the fixture details\n",
    "    fixtures = fixtures_soup.find_all('div', class_='fixture-item')\n",
    "    for fixture in fixtures:\n",
    "        series.append(fixture.find('div', class_='fixture-item__series').text.strip() if fixture.find('div', class_='fixture-item__series') else '-')\n",
    "        places.append(fixture.find('div', class_='fixture-item__venue').text.strip() if fixture.find('div', class_='fixture-item__venue') else '-')\n",
    "        date_time = fixture.find('div', class_='fixture-item__datetime').text.strip() if fixture.find('div', class_='fixture-item__datetime') else '-'\n",
    "        if date_time and '|' in date_time:\n",
    "            date, time = date_time.split('|')\n",
    "            dates.append(date.strip())\n",
    "            times.append(time.strip())\n",
    "        else:\n",
    "            dates.append(date_time)\n",
    "            times.append('-')\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_fixtures = pd.DataFrame({\n",
    "        'Series': series,\n",
    "        'Place': places,\n",
    "        'Date': dates,\n",
    "        'Time': times\n",
    "    })\n",
    "\n",
    "    df_fixtures.head()\n",
    "else:\n",
    "    df_fixtures = pd.DataFrame(columns=['Series', 'Place', 'Date', 'Time'])\n",
    "    print(\"Could not find the international fixtures page link.\")\n",
    "    \n",
    "df_fixtures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f6fabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the State-wise GDP page link.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19) - at current prices</th>\n",
       "      <th>GSDP(19-20) - at current prices</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rank, State, GSDP(18-19) - at current prices, GSDP(19-20) - at current prices, Share(18-19), GDP($ billion)]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 3 - URL of the Statistics Times website\n",
    "base_url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Request the home page\n",
    "response = requests.get(base_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the home page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_page_link = None\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if 'economy' in a['href']:\n",
    "        economy_page_link = a['href']\n",
    "        break\n",
    "\n",
    "# If the economy page link is found, request that page\n",
    "if economy_page_link:\n",
    "    economy_url = base_url + economy_page_link\n",
    "    economy_response = requests.get(economy_url)\n",
    "    economy_response.raise_for_status()\n",
    "    \n",
    "    # Parse the economy page content\n",
    "    economy_soup = BeautifulSoup(economy_response.content, 'html.parser')\n",
    "    \n",
    "    # Find the link to the State-wise GDP page\n",
    "    gdp_page_link = None\n",
    "    for a in economy_soup.find_all('a', href=True):\n",
    "        if 'india/indian-states-gdp.php' in a['href']:\n",
    "            gdp_page_link = a['href']\n",
    "            break\n",
    "    \n",
    "    # If the GDP page link is found, request that page\n",
    "    if gdp_page_link:\n",
    "        gdp_url = base_url + gdp_page_link\n",
    "        gdp_response = requests.get(gdp_url)\n",
    "        gdp_response.raise_for_status()\n",
    "        \n",
    "        # Parse the GDP page content\n",
    "        gdp_soup = BeautifulSoup(gdp_response.content, 'html.parser')\n",
    "        \n",
    "        # Find the table in the page\n",
    "        table = gdp_soup.find('table', {'id': 'table_id'})\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        ranks = []\n",
    "        states = []\n",
    "        gsdp_18_19 = []\n",
    "        gsdp_19_20 = []\n",
    "        share_18_19 = []\n",
    "        gdp_billion = []\n",
    "        \n",
    "        # Extract data from the table\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            ranks.append(cells[0].text.strip() if len(cells) > 0 else '-')\n",
    "            states.append(cells[1].text.strip() if len(cells) > 1 else '-')\n",
    "            gsdp_18_19.append(cells[2].text.strip() if len(cells) > 2 else '-')\n",
    "            gsdp_19_20.append(cells[3].text.strip() if len(cells) > 3 else '-')\n",
    "            share_18_19.append(cells[4].text.strip() if len(cells) > 4 else '-')\n",
    "            gdp_billion.append(cells[5].text.strip() if len(cells) > 5 else '-')\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        df_gdp = pd.DataFrame({\n",
    "            'Rank': ranks,\n",
    "            'State': states,\n",
    "            'GSDP(18-19) - at current prices': gsdp_18_19,\n",
    "            'GSDP(19-20) - at current prices': gsdp_19_20,\n",
    "            'Share(18-19)': share_18_19,\n",
    "            'GDP($ billion)': gdp_billion\n",
    "        })\n",
    "\n",
    "        df_gdp.head()\n",
    "    else:\n",
    "        df_gdp = pd.DataFrame(columns=['Rank', 'State', 'GSDP(18-19) - at current prices', 'GSDP(19-20) - at current prices', 'Share(18-19)', 'GDP($ billion)'])\n",
    "        print(\"Could not find the State-wise GDP page link.\")\n",
    "else:\n",
    "    df_gdp = pd.DataFrame(columns=['Rank', 'State', 'GSDP(18-19) - at current prices', 'GSDP(19-20) - at current prices', 'Share(18-19)', 'GDP($ billion)'])\n",
    "    print(\"Could not find the economy page link.\")\n",
    "    \n",
    "df_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0038aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>The distributed financial transactions databas...</td>\n",
       "      <td>0</td>\n",
       "      <td>Zig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>An extremely fast Python package installer and...</td>\n",
       "      <td>0</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>18 Lessons, Get Started Building with Generati...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>Elden Ring Save Editor. Compatible with PC and...</td>\n",
       "      <td>0</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>☄🌌️ The minimal, blazing-fast, and infinitely ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Repository Title                             Repository Description  \\\n",
       "0                -  The distributed financial transactions databas...   \n",
       "1                -  An extremely fast Python package installer and...   \n",
       "2                -  18 Lessons, Get Started Building with Generati...   \n",
       "3                -  Elden Ring Save Editor. Compatible with PC and...   \n",
       "4                -  ☄🌌️ The minimal, blazing-fast, and infinitely ...   \n",
       "\n",
       "   Contributors Count     Language Used  \n",
       "0                   0               Zig  \n",
       "1                   0              Rust  \n",
       "2                   0  Jupyter Notebook  \n",
       "3                   0              Rust  \n",
       "4                   0              Rust  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 4 - URL of the GitHub trending repositories page\n",
    "url = \"https://github.com/trending\"\n",
    "\n",
    "# Request the trending repositories page\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "titles = []\n",
    "descriptions = []\n",
    "contributors_counts = []\n",
    "languages = []\n",
    "\n",
    "# Extract data from the page\n",
    "repos = soup.find_all('article', class_='Box-row')\n",
    "for repo in repos:\n",
    "    # Title\n",
    "    title_tag = repo.find('h1', class_='h3 lh-condensed')\n",
    "    if title_tag:\n",
    "        title = title_tag.text.strip()\n",
    "        titles.append(title)\n",
    "    else:\n",
    "        titles.append('-')\n",
    "    \n",
    "    # Description\n",
    "    description_tag = repo.find('p', class_='col-9 color-fg-muted my-1 pr-4')\n",
    "    description = description_tag.text.strip() if description_tag else '-'\n",
    "    descriptions.append(description)\n",
    "    \n",
    "    # Contributors count\n",
    "    contributors_tag = repo.find_all('a', class_='Link--muted d-inline-block mr-3')\n",
    "    contributors_count = len(contributors_tag)\n",
    "    contributors_counts.append(contributors_count)\n",
    "    \n",
    "    # Language used\n",
    "    language_tag = repo.find('span', itemprop='programmingLanguage')\n",
    "    language = language_tag.text.strip() if language_tag else '-'\n",
    "    languages.append(language)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_trending_repos = pd.DataFrame({\n",
    "    'Repository Title': titles,\n",
    "    'Repository Description': descriptions,\n",
    "    'Contributors Count': contributors_counts,\n",
    "    'Language Used': languages\n",
    "})\n",
    "\n",
    "df_trending_repos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b73fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m     weeks_on_board_tag \u001b[38;5;241m=\u001b[39m chart_item\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-label\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-weeks-on-chart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[0;32m     47\u001b[0m     weeks_on_board \u001b[38;5;241m=\u001b[39m weeks_on_board_tag\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m weeks_on_board_tag \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 48\u001b[0m     weeks_on_board\u001b[38;5;241m.\u001b[39mappend(weeks_on_board)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     51\u001b[0m df_hot_100 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSong Name\u001b[39m\u001b[38;5;124m'\u001b[39m: songs,\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtist Name\u001b[39m\u001b[38;5;124m'\u001b[39m: artists,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeeks on Board\u001b[39m\u001b[38;5;124m'\u001b[39m: weeks_on_board\n\u001b[0;32m     57\u001b[0m })\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 5 - Direct URL to the Hot 100 chart page\n",
    "hot_100_url = \"https://www.billboard.com/charts/hot-100/\"\n",
    "\n",
    "# Request the Hot 100 page\n",
    "response = requests.get(hot_100_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the Hot 100 page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "songs = []\n",
    "artists = []\n",
    "last_week_ranks = []\n",
    "peak_ranks = []\n",
    "weeks_on_board = []\n",
    "\n",
    "# Extract data from the Hot 100 page\n",
    "chart_list = soup.find_all('li', class_='o-chart-results-list__item')\n",
    "for chart_item in chart_list:\n",
    "    # Song name\n",
    "    song_name_tag = chart_item.find('h3', id='title-of-a-story')\n",
    "    song_name = song_name_tag.text.strip() if song_name_tag else '-'\n",
    "    songs.append(song_name)\n",
    "    \n",
    "    # Artist name\n",
    "    artist_name_tag = chart_item.find('span', class_='c-label')\n",
    "    artist_name = artist_name_tag.text.strip() if artist_name_tag else '-'\n",
    "    artists.append(artist_name)\n",
    "    \n",
    "    # Last week rank\n",
    "    last_week_rank_tag = chart_item.find('span', class_='c-label', attrs={'data-rank-last-week': True})\n",
    "    last_week_rank = last_week_rank_tag.text.strip() if last_week_rank_tag else '-'\n",
    "    last_week_ranks.append(last_week_rank)\n",
    "    \n",
    "    # Peak rank\n",
    "    peak_rank_tag = chart_item.find('span', class_='c-label', attrs={'data-rank-peak': True})\n",
    "    peak_rank = peak_rank_tag.text.strip() if peak_rank_tag else '-'\n",
    "    peak_ranks.append(peak_rank)\n",
    "    \n",
    "    # Weeks on board\n",
    "    weeks_on_board_tag = chart_item.find('span', class_='c-label', attrs={'data-weeks-on-chart': True})\n",
    "    weeks_on_board = weeks_on_board_tag.text.strip() if weeks_on_board_tag else '-'\n",
    "    weeks_on_board.append(weeks_on_board)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_hot_100 = pd.DataFrame({\n",
    "    'Song Name': songs,\n",
    "    'Artist Name': artists,\n",
    "    'Last Week Rank': last_week_ranks,\n",
    "    'Peak Rank': peak_ranks,\n",
    "    'Weeks on Board': weeks_on_board\n",
    "})\n",
    "\n",
    "df_hot_100.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0df820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\workdata\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\workdata\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in d:\\workdata\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workdata\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workdata\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workdata\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workdata\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\workdata\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\workdata\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\workdata\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\workdata\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\workdata\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\workdata\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7680807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Book Name                                Author Name   Volumes Sold  \\\n",
       "0         1                          Da Vinci Code,The     Brown, Dan   \n",
       "1         2       Harry Potter and the Deathly Hallows  Rowling, J.K.   \n",
       "2         3   Harry Potter and the Philosopher's Stone  Rowling, J.K.   \n",
       "3         4  Harry Potter and the Order of the Phoenix  Rowling, J.K.   \n",
       "4         5                       Fifty Shades of Grey   James, E. L.   \n",
       "\n",
       "   Publisher         Genre  \n",
       "0  5,094,805    Transworld  \n",
       "1  4,475,152    Bloomsbury  \n",
       "2  4,200,654    Bloomsbury  \n",
       "3  4,179,479    Bloomsbury  \n",
       "4  3,758,936  Random House  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 6 - URL of the Guardian page with the highest selling novels\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Find the table with the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract data from the table\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) >= 5:  # Ensure there are at least 5 columns\n",
    "        book_names.append(cols[0].text.strip())\n",
    "        author_names.append(cols[1].text.strip())\n",
    "        volumes_sold.append(cols[2].text.strip())\n",
    "        publishers.append(cols[3].text.strip())\n",
    "        genres.append(cols[4].text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "df_best_selling_novels = pd.DataFrame({\n",
    "    'Book Name': book_names,\n",
    "    'Author Name': author_names,\n",
    "    'Volumes Sold': volumes_sold,\n",
    "    'Publisher': publishers,\n",
    "    'Genre': genres\n",
    "})\n",
    "\n",
    "df_best_selling_novels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4f6517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Year Span, Genre, Run Time, Ratings, Votes]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 7 - Correct URL of an IMDb page with a popular TV series list\n",
    "url = \"https://www.imdb.com/chart/toptv/\"\n",
    "\n",
    "# Headers to mimic a web browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Request the page with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "run_times = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Extract data from the page\n",
    "items = soup.find_all('tr')[1:]  # Skip the header row\n",
    "for item in items:\n",
    "    # Name\n",
    "    name_tag = item.find('td', class_='titleColumn')\n",
    "    if name_tag:\n",
    "        name = name_tag.a.text.strip()\n",
    "        names.append(name)\n",
    "    \n",
    "    # Year span\n",
    "    year_span_tag = name_tag.span if name_tag else None\n",
    "    year_span = year_span_tag.text.strip(\"()\") if year_span_tag else '-'\n",
    "    year_spans.append(year_span)\n",
    "    \n",
    "    # Genre (not available in this table directly)\n",
    "    genres.append('-')  # Placeholder\n",
    "    \n",
    "    # Run time (not available in this table directly)\n",
    "    run_times.append('-')  # Placeholder\n",
    "    \n",
    "    # Ratings\n",
    "    rating_tag = item.find('td', class_='ratingColumn imdbRating')\n",
    "    rating = rating_tag.strong.text.strip() if rating_tag and rating_tag.strong else '-'\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Votes (not available in this table directly)\n",
    "    votes.append('-')  # Placeholder\n",
    "\n",
    "# Create a DataFrame\n",
    "df_most_watched_tv_series = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Year Span': year_spans,\n",
    "    'Genre': genres,\n",
    "    'Run Time': run_times,\n",
    "    'Ratings': ratings,\n",
    "    'Votes': votes\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_most_watched_tv_series.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a3488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Question 8  - Base URL of the UCI Machine Learning Repository\n",
    "base_url = \"https://archive.ics.uci.edu/ml/index.php\"\n",
    "\n",
    "# Headers to mimic a web browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Request the home page\n",
    "response = requests.get(base_url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the home page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the link to the \"View All Datasets\" page by looking for a link containing the keyword 'datasets'\n",
    "all_datasets_link = None\n",
    "for a_tag in soup.find_all('a'):\n",
    "    if 'datasets' in a_tag.get('href', ''):\n",
    "        all_datasets_link = a_tag\n",
    "        break\n",
    "\n",
    "if all_datasets_link is None:\n",
    "    raise Exception(\"Couldn't find the link to the datasets page\")\n",
    "\n",
    "# Debug: print the found link\n",
    "print(\"Found link:\", all_datasets_link)\n",
    "\n",
    "all_datasets_url = \"https://archive.ics.uci.edu/ml/\" + all_datasets_link['href']\n",
    "\n",
    "# Debug: print the constructed URL\n",
    "print(\"Constructed URL:\", all_datasets_url)\n",
    "\n",
    "# Request the \"View All Datasets\" page\n",
    "response = requests.get(all_datasets_url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the \"View All Datasets\" page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "dataset_names = []\n",
    "data_types = []\n",
    "tasks = []\n",
    "attribute_types = []\n",
    "num_instances = []\n",
    "num_attributes = []\n",
    "years = []\n",
    "\n",
    "# Extract data from the datasets table\n",
    "table = soup.find('table', {'border': '1'})\n",
    "rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    dataset_names.append(cols[0].text.strip())\n",
    "    data_types.append(cols[1].text.strip())\n",
    "    tasks.append(cols[2].text.strip())\n",
    "    attribute_types.append(cols[3].text.strip())\n",
    "    num_instances.append(cols[4].text.strip())\n",
    "    num_attributes.append(cols[5].text.strip())\n",
    "    years.append(cols[6].text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "df_datasets = pd.DataFrame({\n",
    "    'Dataset Name': dataset_names,\n",
    "    'Data Type': data_types,\n",
    "    'Task': tasks,\n",
    "    'Attribute Type': attribute_types,\n",
    "    'No of Instances': num_instances,\n",
    "    'No of Attributes': num_attributes,\n",
    "    'Year': years\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_datasets.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
