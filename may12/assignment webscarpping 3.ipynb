{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a6a45e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a product name: mobile\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#Question 1\n",
    "def search_amazon(product_name):\n",
    "    url = f\"https://www.amazon.in/s?k={product_name}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    products = []\n",
    "\n",
    "    for result in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        title_tag = result.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "        link_tag = result.find(\"a\", {\"class\": \"a-link-normal a-text-normal\"})\n",
    "\n",
    "        if title_tag and link_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            link = \"https://www.amazon.in\" + link_tag[\"href\"]\n",
    "            products.append({\"title\": title, \"link\": link})\n",
    "\n",
    "    return products\n",
    "\n",
    "product_name = input(\"Enter a product name: \")\n",
    "products = search_amazon(product_name)\n",
    "for product in products:\n",
    "    print(f\"Title: {product['title']}, Link: {product['link']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99a9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a product name: mobile\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m products\n\u001b[0;32m     33\u001b[0m product_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a product name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m products \u001b[38;5;241m=\u001b[39m scrape_amazon(product_name)\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(products)\n\u001b[0;32m     36\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124mlip\u001b[39m\u001b[38;5;130;01m\\a\u001b[39;00m\u001b[38;5;124mssignment webscrap june20th\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mscrape_amazon\u001b[1;34m(product_name)\u001b[0m\n\u001b[0;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-component-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms-search-result\u001b[39m\u001b[38;5;124m\"\u001b[39m}):\n\u001b[1;32m---> 13\u001b[0m     product_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.amazon.in\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m result\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-link-normal s-underline-text s-underline-link-text s-link-style\u001b[39m\u001b[38;5;124m\"\u001b[39m})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m     product_page_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(product_url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     15\u001b[0m     product_page_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(product_page_response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#question 2\n",
    "def scrape_amazon(product_name):\n",
    "    products = []\n",
    "    for page in range(1, 4):  # scrape first 3 pages\n",
    "        url = f\"https://www.amazon.in/s?k={product_name}&page={page}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        for result in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "            product_url = \"https://www.amazon.in\" + result.find(\"a\", {\"class\": \"a-link-normal s-underline-text s-underline-link-text s-link-style\"})[\"href\"]\n",
    "            product_page_response = requests.get(product_url, headers=headers)\n",
    "            product_page_soup = BeautifulSoup(product_page_response.content, \"html.parser\")\n",
    "            brand_name = product_page_soup.find(\"a\", {\"id\": \"bylineInfo\"}).text.strip() if product_page_soup.find(\"a\", {\"id\": \"bylineInfo\"}) else \"-\"\n",
    "            product_name = product_page_soup.find(\"span\", {\"id\": \"productTitle\"}).text.strip() if product_page_soup.find(\"span\", {\"id\": \"productTitle\"}) else \"-\"\n",
    "            price = product_page_soup.find(\"span\", {\"id\": \"priceblock_ourprice\"}).text.strip() if product_page_soup.find(\"span\", {\"id\": \"priceblock_ourprice\"}) else \"-\"\n",
    "            return_exchange = product_page_soup.find(\"span\", {\"class\": \"a-size-base a-color-base\"}).text.strip() if product_page_soup.find(\"span\", {\"class\": \"a-size-base a-color-base\"}) else \"-\"\n",
    "            expected_delivery = product_page_soup.find(\"span\", {\"id\": \"delivery-message\"}).text.strip() if product_page_soup.find(\"span\", {\"id\": \"delivery-message\"}) else \"-\"\n",
    "            availability = product_page_soup.find(\"span\", {\"id\": \"availability\"}).text.strip() if product_page_soup.find(\"span\", {\"id\": \"availability\"}) else \"-\"\n",
    "            products.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Name of the Product\": product_name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "    return products\n",
    "\n",
    "product_name = input(\"Enter a product name: \")\n",
    "products = scrape_amazon(product_name)\n",
    "df = pd.DataFrame(products)\n",
    "df.to_csv(f\"D:\\flip\\assignment webscrap june20th\\''{product_name}.csv\", index=False)\n",
    "print(f\"Data saved to {product_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5381173c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Fetch and download images for each search term\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m search_terms:\n\u001b[1;32m---> 60\u001b[0m     image_urls \u001b[38;5;241m=\u001b[39m fetch_image_urls(term)\n\u001b[0;32m     61\u001b[0m     download_images(term, image_urls)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Close Chrome driver\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mfetch_image_urls\u001b[1;34m(term)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_image_urls\u001b[39m(term):\n\u001b[0;32m     32\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://images.google.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     search_box \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m     search_box\u001b[38;5;241m.\u001b[39msend_keys(term)\n\u001b[0;32m     35\u001b[0m     search_box\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mRETURN)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_name'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "# Set up Chrome options--Question3\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Set up search terms\n",
    "search_terms = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "\n",
    "# Set up image count\n",
    "image_count = 10\n",
    "\n",
    "# Create a directory for each search term\n",
    "for term in search_terms:\n",
    "    os.makedirs(term, exist_ok=True)\n",
    "\n",
    "# Function to fetch image URLs\n",
    "def fetch_image_urls(term):\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys(term)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "    image_urls = []\n",
    "    while len(image_urls) < image_count:\n",
    "        thumbnails = driver.find_elements_by_css_selector(\"img.rg_i\")\n",
    "        for thumbnail in thumbnails:\n",
    "            thumbnail.click()\n",
    "            time.sleep(1)\n",
    "            actual_image = driver.find_element_by_css_selector(\"img.n3VNCb\")\n",
    "            image_url = actual_image.get_attribute(\"src\")\n",
    "            if image_url:\n",
    "                image_urls.append(image_url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    return image_urls\n",
    "\n",
    "# Function to download images\n",
    "def download_images(term, image_urls):\n",
    "    for i, url in enumerate(image_urls):\n",
    "        filename = f\"{term}/{i+1}.jpg\"\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Fetch and download images for each search term\n",
    "for term in search_terms:\n",
    "    image_urls = fetch_image_urls(term)\n",
    "    download_images(term, image_urls)\n",
    "\n",
    "# Close Chrome driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787ac167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a smartphone name: samsung\n",
      "Data saved to samsung.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#Question4\n",
    "def scrape_flipkart(smartphone_name):\n",
    "    url = f\"https://www.flipkart.com/search?q={smartphone_name}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    products = []\n",
    "    for result in soup.find_all(\"div\", {\"class\": \"_2kHMtA\"}):\n",
    "        product_url = \"https://www.flipkart.com\" + result.find(\"a\", {\"class\": \"_1fQZEK\"})[\"href\"]\n",
    "        product_page_response = requests.get(product_url, headers=headers)\n",
    "        product_page_soup = BeautifulSoup(product_page_response.content, \"html.parser\")\n",
    "        brand_name = product_page_soup.find(\"span\", {\"class\": \"_2J4LW6\"}).text.strip() if product_page_soup.find(\"span\", {\"class\": \"_2J4LW6\"}) else \"-\"\n",
    "        smartphone_name = product_page_soup.find(\"span\", {\"class\": \"_35KyD6\"}).text.strip() if product_page_soup.find(\"span\", {\"class\": \"_35KyD6\"}) else \"-\"\n",
    "        colour = product_page_soup.find(\"div\", {\"class\": \"_3FxgXg\"}).text.strip() if product_page_soup.find(\"div\", {\"class\": \"_3FxgXg\"}) else \"-\"\n",
    "        specs = product_page_soup.find_all(\"li\", {\"class\": \"_2ascJb\"})\n",
    "        ram = \"-\"\n",
    "        storage = \"-\"\n",
    "        primary_camera = \"-\"\n",
    "        secondary_camera = \"-\"\n",
    "        display_size = \"-\"\n",
    "        battery_capacity = \"-\"\n",
    "        for spec in specs:\n",
    "            if \"RAM\" in spec.text:\n",
    "                ram = spec.text.split(\":\")[1].strip()\n",
    "            elif \"Storage\" in spec.text:\n",
    "                storage = spec.text.split(\":\")[1].strip()\n",
    "            elif \"Primary Camera\" in spec.text:\n",
    "                primary_camera = spec.text.split(\":\")[1].strip()\n",
    "            elif \"Secondary Camera\" in spec.text:\n",
    "                secondary_camera = spec.text.split(\":\")[1].strip()\n",
    "            elif \"Display Size\" in spec.text:\n",
    "                display_size = spec.text.split(\":\")[1].strip()\n",
    "            elif \"Battery Capacity\" in spec.text:\n",
    "                battery_capacity = spec.text.split(\":\")[1].strip()\n",
    "        price = product_page_soup.find(\"div\", {\"class\": \"_1vC4OE _2rQ-NK\"}).text.strip() if product_page_soup.find(\"div\", {\"class\": \"_1vC4OE _2rQ-NK\"}) else \"-\"\n",
    "        products.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Smartphone Name\": smartphone_name,\n",
    "            \"Colour\": colour,\n",
    "            \"RAM\": ram,\n",
    "            \"Storage(ROM)\": storage,\n",
    "            \"Primary Camera\": primary_camera,\n",
    "            \"Secondary Camera\": secondary_camera,\n",
    "            \"Display Size\": display_size,\n",
    "            \"Battery Capacity\": battery_capacity,\n",
    "            \"Price\": price,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "    return products\n",
    "\n",
    "smartphone_name = input(\"Enter a smartphone name: \")\n",
    "products = scrape_flipkart(smartphone_name)\n",
    "df = pd.DataFrame(products)\n",
    "df.to_csv(f\"{smartphone_name}.csv\", index=False)\n",
    "print(f\"Data saved to {smartphone_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01941cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\workdata\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workdata\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workdata\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workdata\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workdata\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1510e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a city: London\n",
      "Error fetching data: REQUEST_DENIED\n",
      "Could not retrieve geospatial coordinates.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#question 5\n",
    "def get_geospatial_coordinates(city, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"address\": city,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['status'] == 'OK':\n",
    "        location = data['results'][0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(f\"Error fetching data: {data['status']}\")\n",
    "        return None, None\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual Google Maps Geocoding API key\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "city = input(\"Enter a city: \")\n",
    "latitude, longitude = get_geospatial_coordinates(city, api_key)\n",
    "\n",
    "if latitude and longitude:\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Could not retrieve geospatial coordinates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc51194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#question 6\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/laptops/best-gaming-laptops-in-india.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    laptops = soup.find_all(\"div\", {\"class\": \"listing-item\"})\n",
    "    data = []\n",
    "    for laptop in laptops:\n",
    "        brand_name = laptop.find(\"h2\", {\"class\": \"title\"}).text.strip()\n",
    "        product_name = laptop.find(\"h3\", {\"class\": \"subtitle\"}).text.strip()\n",
    "        price = laptop.find(\"span\", {\"class\": \"price\"}).text.strip()\n",
    "        availability = laptop.find(\"span\", {\"class\": \"availability\"}).text.strip()\n",
    "        product_url = laptop.find(\"a\", {\"class\": \"listing-link\"})[\"href\"]\n",
    "        specs = laptop.find(\"ul\", {\"class\": \"specs\"}).find_all(\"li\")\n",
    "        specs_dict = {}\n",
    "        for spec in specs:\n",
    "            key, value = spec.text.strip().split(\": \")\n",
    "            specs_dict[key] = value\n",
    "        data.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Product Name\": product_name,\n",
    "            \"Price\": price,\n",
    "            \"Availability\": availability,\n",
    "            \"Product URL\": product_url,\n",
    "            \"Processor\": specs_dict.get(\"Processor\", \"-\"),\n",
    "            \"Graphics Card\": specs_dict.get(\"Graphics Card\", \"-\"),\n",
    "            \"RAM\": specs_dict.get(\"RAM\", \"-\"),\n",
    "            \"Storage\": specs_dict.get(\"Storage\", \"-\"),\n",
    "            \"Display\": specs_dict.get(\"Display\", \"-\"),\n",
    "            \"Refresh Rate\": specs_dict.get(\"Refresh Rate\", \"-\")\n",
    "        })\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "\n",
    "scrape_gaming_laptops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4e76c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Scrape and display the data\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m billionaires_df \u001b[38;5;241m=\u001b[39m scrape_forbes_billionaires()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(billionaires_df)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Optionally, save to a CSV file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m, in \u001b[0;36mscrape_forbes_billionaires\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Locate the table containing the billionaire data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-container\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 12\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-row\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows[\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# Skipping the header row\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#Question 7\n",
    "def scrape_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Locate the table containing the billionaire data\n",
    "    table = soup.find('div', {'class': 'table-container'})\n",
    "    rows = table.find_all('div', {'class': 'table-row'})\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # Skipping the header row\n",
    "        rank = row.find('div', {'class': 'Table_rank__X4MKf'}).get_text(strip=True)\n",
    "        name = row.find('div', {'class': 'Table_personName__Bus2E'}).get_text(strip=True)\n",
    "        net_worth = row.find('div', {'class': 'Table_finalWorth__UZA6k'}).get_text(strip=True)\n",
    "        age = row.find('div', {'class': 'age'}).get_text(strip=True)\n",
    "        citizenship = row.find('div', {'class': 'countryOfCitizenship'}).get_text(strip=True)\n",
    "        source = row.find('div', {'class': 'source'}).get_text(strip=True)\n",
    "        industry = row.find('div', {'class': 'category'}).get_text(strip=True)\n",
    "        \n",
    "        data.append({\n",
    "            'Rank': rank,\n",
    "            'Name': name,\n",
    "            'Net worth': net_worth,\n",
    "            'Age': age,\n",
    "            'Citizenship': citizenship,\n",
    "            'Source': source,\n",
    "            'Industry': industry\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Scrape and display the data\n",
    "billionaires_df = scrape_forbes_billionaires()\n",
    "print(billionaires_df)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "billionaires_df.to_csv('D:\\flip\\assignment webscrap june20th\\forbes_billionaires.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbc8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Obtaining dependency information for google-api-python-client from https://files.pythonhosted.org/packages/26/05/84740f006d5283f6500d97f7877bbc8928effc5e6d79018107fccb0743e1/google_api_python_client-2.134.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_api_python_client-2.134.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for httplib2<1.dev0,>=0.19.0 from https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 from https://files.pythonhosted.org/packages/28/b1/2f3dc4c814a83e618a0a624f37bcc885fd8c2f2af6f0e547288d2c4f64c2/google_auth-2.30.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth-httplib2<1.0.0,>=0.2.0 from https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 from https://files.pythonhosted.org/packages/2d/ed/e514e0c59cdf1a469b1a1ab21b77698d0692adaa7cbc920c3a0b287e8493/google_api_core-2.19.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Obtaining dependency information for uritemplate<5,>=3.0.1 from https://files.pythonhosted.org/packages/81/c0/7461b49cd25aeece13766f02ee576d1db528f1c37ce69aee300e075b485b/uritemplate-4.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/98/87/1608d23bb9879694579fff5dc56d60e3d48e012fd08670f140cf82f6cf26/googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for proto-plus<2.0.0dev,>=1.22.3 from https://files.pythonhosted.org/packages/7c/6f/db31f0711c0402aa477257205ce7d29e86a75cb52cd19f7afb585f75cda0/proto_plus-1.24.0-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\workdata\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\workdata\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\workdata\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\workdata\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workdata\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workdata\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workdata\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workdata\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Downloading google_api_python_client-2.134.0-py2.py3-none-any.whl (11.9 MB)\n",
      "   ---------------------------------------- 0.0/11.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/11.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.7/11.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/11.9 MB 22.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.3/11.9 MB 22.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/11.9 MB 20.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.2/11.9 MB 20.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.3/11.9 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.4/11.9 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/11.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/11.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/11.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/11.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/11.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.9/11.9 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.0/139.0 kB ? eta 0:00:00\n",
      "Downloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "   ---------------------------------------- 0.0/193.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 193.7/193.7 kB ? eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.9/96.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 229.2/229.2 kB ? eta 0:00:00\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, cachetools, proto-plus, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.3 google-api-core-2.19.0 google-api-python-client-2.134.0 google-auth-2.30.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.63.1 httplib2-0.22.0 proto-plus-1.24.0 protobuf-4.25.3 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d632dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\workdata\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\workdata\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in d:\\workdata\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workdata\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workdata\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workdata\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workdata\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\workdata\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\workdata\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\workdata\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\workdata\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\workdata\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\workdata\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c92494",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet%2Creplies&videoId=ENTER_VIDEO_ID&key=YOUR_API_KEY&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube_comments.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m video_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENTER_VIDEO_ID\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 37\u001b[0m video_comments(video_id)\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mvideo_comments\u001b[1;34m(video_id)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvideo_comments\u001b[39m(video_id):\n\u001b[0;32m      6\u001b[0m     youtube \u001b[38;5;241m=\u001b[39m build(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m      7\u001b[0m     video_response \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mcommentThreads()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m      8\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet,replies\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m         videoId\u001b[38;5;241m=\u001b[39mvideo_id\n\u001b[1;32m---> 10\u001b[0m     )\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[0;32m     12\u001b[0m     comments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m video_response:\n",
      "File \u001b[1;32mD:\\workdata\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\workdata\\Lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet%2Creplies&videoId=ENTER_VIDEO_ID&key=YOUR_API_KEY&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'YOUR_API_KEY'\n",
    "#question 8\n",
    "def video_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    video_response = youtube.commentThreads().list(\n",
    "        part='snippet,replies',\n",
    "        videoId=video_id\n",
    "    ).execute()\n",
    "\n",
    "    comments = []\n",
    "    while video_response:\n",
    "        for item in video_response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            reply_count = item['snippet']['totalReplyCount']\n",
    "            if reply_count > 0:\n",
    "                for reply in item['replies']['comments']:\n",
    "                    reply = reply['snippet']['textDisplay']\n",
    "                    comments.append({'Comment': comment, 'Reply': reply, 'Upvote': item['snippet']['topLevelComment']['snippet']['likeCount'], 'Time': item['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "            else:\n",
    "                comments.append({'Comment': comment, 'Reply': '-', 'Upvote': item['snippet']['topLevelComment']['snippet']['likeCount'], 'Time': item['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(comments)\n",
    "    df.to_csv('youtube_comments.csv', index=False)\n",
    "\n",
    "video_id = 'ENTER_VIDEO_ID'\n",
    "video_comments(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e706cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#Question 9\n",
    "def scrape_hostels():\n",
    "    url = \"https://www.hostelworld.com/hostels/London\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    hostels = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    hostels_data = []\n",
    "\n",
    "    for hostel in hostels:\n",
    "        name_element = hostel.find('div', {'data-testid': 'title'})\n",
    "        name = name_element.text.strip()\n",
    "\n",
    "        distance_element = hostel.find('span', {'data-testid': 'distance'})\n",
    "        distance = distance_element.text.strip()\n",
    "\n",
    "        ratings_element = hostel.find('div', {'class': 'rating'})\n",
    "        ratings = ratings_element.text.strip()\n",
    "\n",
    "        reviews_element = hostel.find('span', {'data-testid': 'eviews'})\n",
    "        reviews = reviews_element.text.strip()\n",
    "\n",
    "        privates_price_element = hostel.find('span', {'data-testid': 'privates-price'})\n",
    "        privates_price = privates_price_element.text.strip()\n",
    "\n",
    "        dorms_price_element = hostel.find('span', {'data-testid': 'dorms-price'})\n",
    "        dorms_price = dorms_price_element.text.strip()\n",
    "\n",
    "        facilities_element = hostel.find('div', {'data-testid': 'facilities'})\n",
    "        facilities = facilities_element.text.strip()\n",
    "\n",
    "        property_description_element = hostel.find('div', {'data-testid': 'property-description'})\n",
    "        property_description = property_description_element.text.strip()\n",
    "\n",
    "        hostels_data.append({\n",
    "            'Name': name,\n",
    "            'Distance from City Centre': distance,\n",
    "            'Ratings': ratings,\n",
    "            'Total Reviews': reviews,\n",
    "            'Privates from Price': privates_price,\n",
    "            'Dorms from Price': dorms_price,\n",
    "            'Facilities': facilities,\n",
    "            'Property Description': property_description\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(hostels_data)\n",
    "    df.to_csv('hostels_london.csv', index=False)\n",
    "\n",
    "scrape_hostels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edcd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
